# jemdoc: menu{MENU.txt}{zsd.html}
= Zero-Shot Object Detection
Ankan Bansal, [https://ksikka.com/ Karan Sikka]\*, [http://www.grvsharma.com/research.html Gaurav Sharma]^, Rama Chellappa, and Ajay Divakaran\*\n
\* SRI International, Princeton, NJ\n
^ NEC Labs America, Cupertino, CA

~~~
{}{img_left}{projects/zsd/system_diagram.png}{System}{900px}{300px}

~~~
    
{{<font size=2><b>Fig. 1:</b> We highlight the task of zero-shot object detection where object classes “arm”, 
“hand”, and “shirt” are observed (seen) during training, while classes “skirt”, and
“shoulder” are not seen. These unseen classes are localized by our approach that leverages
semantic relationships, obtained via word embeddings, between seen and unseen classes
along with the proposed zero-shot detection framework. The example has been generated by
our model on images from VisualGenome dataset.</font>}}


\n
*Abstract*: In this work, we introduce and tackle the problem of zero-shot object detection (ZSD), which
aims to detect object classes which are not observed during training. We work with a
challenging set of object classes, not restricting ourselves to similar and\/or fine-grained
categories cf. prior works on zero-shot classification. We follow a principled approach by
first adapting visual-semantic embeddings for ZSD. We then discuss the problems associated
with selecting a background class and motivate two background-aware approaches for learning 
robust detectors. One of these models uses a fixed background class and the other is based 
on iterative latent assignments. We also outline the challenge associated with using a 
limited number of training classes and propose a solution based on dense sampling of the 
semantic label space using auxiliary data with a large number of categories. We propose 
novel splits of two standard detection datasets – MSCOCO and VisualGenome, and discuss 
extensive empirical results to highlight the benefits of the proposed methods. We provide 
useful insights into the algorithm and conclude by posing some open questions to encourage 
further research.

== Results
*We highlight the results obtained on unseen classes in the following figure.*\n
#*+Put Results Tables Here+*
#~~~
#{}{table}{Results}
#  | | *MSCOCO* | *Visual Genome* ||
#ZSD Method  | BG-aware | \#classes | IoU | \#classes | IoU ||
# | | S | U | O | 0.4 | 0.5 | 0.6 | S | U | O | 0.4 | 0.5 | 0.6 ||
#Baseline | | 48 | 17 | 0 | 34.36 | 22.14 | 11.31 | 478 | 130 | 0 | 8.19 | 5.19 | 2.63 ||
#Baseline | | 48 | 17 | 0 | 34.36 | 22.14 | 11.31 | 478 | 130 | 0 | 8.19 | 5.19 | 2.63 
#~~~

~~~
{}{img_left}{projects/zsd/results1.png}{results}{1200px}{320px}
~~~

~~~
{}{img_left}{projects/zsd/results2.png}{results}{1200px}{320px}
~~~

~~~
{}{img_left}{projects/zsd/results3.png}{results}{1200px}{320px}
~~~

~~~
{}{img_left}{projects/zsd/results4.png}{results}{1200px}{320px}
~~~

{{<font size=2><b>Fig. 2:</b> This figure shows some detections made by our background-aware methods. We
have used Latent Assignment Based (LAB) model for VisualGenome (rows 1-2) and the Static
Background (SB) model (rows 3-4) for MSCOCO. Reasonable detections are shown in blue
and two failure cases in red. This figure highlights the effectiveness of our methods in
being able to detect unseen classes in a zero-shot setting.</font>}}


== Downloads
We are releasing our seen and unseen class names and train and test splits. This is an attempt to
standardize the work done in this area. 
=== VG
*Train and Test splits*
- [https://obj.umiacs.umd.edu/zsd_files/vg_train_list.json.tar.gz Train Bounding Boxes]
- [files/vg_test_list.txt Test File List]

*Seen and Unseen classes*
- [files/vg_seen_classes.json Seen Classes]
- [files/vg_unseen_classes.json Unseen Classes]

*Synset-Word Dictionary*
- [files/vg_synset_word_dict.json Synset-Word Dict]

=== MSCOCO
*Train and Test splits*
- [https://obj.umiacs.umd.edu/zsd_files/mscoco_train_list.json.tar.gz Train Bounding Boxes] (Note that these bounding boxes are from about 
44,000 unique images. We started with about 73k images and after removing those with unseen classes, were left with 44k images.)
- [files/mscoco_test_list.txt Test File List]

*Seen and Unseen classes*
- [files/mscoco_seen_classes.json Seen Classes]
- [files/mscoco_unseen_classes.json Unseen Classes]

*Synset-Word Dictionary*
- [files/mscoco_synset_word_dict.json Synset-Word Dict]


== Paper
Our paper is available [https://openaccess.thecvf.com/content_ECCV_2018/papers/Ankan_Bansal_Zero-Shot_Object_Detection_ECCV_2018_paper.pdf here]. I also wrote a short [https://computervizion.blogspot.com/2018/04/zero-shot-object-detection.html blog post] about the paper.

If you found the paper and data useful, please consider citing our paper using the bibtex:\n

~~~
{}{}
@inproceedings{bansal2018zero,
  title={Zero-shot object detection},
  author={Bansal, Ankan and Sikka, Karan and Sharma, Gaurav and Chellappa, Rama and Divakaran, Ajay},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={384--400},
  year={2018}
}
~~~


*Acknowledgments*

This project is sponsored by the Air Force Research Laboratory (AFRL) and Defense Advanced
Research Projects Agency (DARPA) under the contract number USAF\/AFMC AFRL FA8750-16-C-0158.\n
*Disclaimer*: The views, opinions, and\/or findings expressed are those of the author(s) and
should not be interpreted as representing the official views or policies of the Department of
Defense or the U.S. Government.

This work is also supported by the Intelligence Advanced Research Projects Activity (IARPA)
    via Department of Interior/Interior Business Center (DOI\/IBC) contract number D17PC00345. The
    U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not
    withstanding any copyright annotation thereon.\n
*Disclaimer*: The views and conclusions contained
    herein are those of the authors and should not be interpreted as necessarily representing the
    official policies or endorsements, either expressed or implied of IARPA, DOI\/IBC or the U.S.
    Government.
