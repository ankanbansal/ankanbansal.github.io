<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Image-Set Visual Question Answering</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Ankan Bansal</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="projects.html">Projects</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="A.Bansal_Resume.pdf">Resume</a></div>
<div class="menu-item"><a href="contact.html">Contact</a></div>
<div class="menu-item"><a href="https://computervizion.blogspot.com/">Blog</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Image-Set Visual Question Answering</h1>
<div id="subtitle">Ankan Bansal, Yuting Zhang, and Rama Chellappa<br /></div>
</div>
<table class="imgtable"><tr><td>
<img src="projects/isvqa/more_examples.PNG" alt="Examples" width="900px" height="200px" />&nbsp;</td>
<td align="left"></td></tr></table>
<p><font size=2><b>Fig. 1:</b> Some examples from our ISVQA dataset.</font></p>
<p><br />
<b>Abstract</b>: We introduce the task of Image-Set Visual Question Answering
(ISVQA), which generalizes the commonly studied single-image VQA problem to
multi-image settings. Taking a natural language question and a set of images as
input, it aims to answer the question based on the contents of the images. The
questions can be about objects and relationships in one or more images or about
the entire scene depicted by the image set. To enable research on this new
topic, we introduce two ISVQA datasets - indoor and outdoor scenes. They
simulate the real-world scenarios of indoor image collections and multiple
car-mounted cameras, respectively. The indoor-scene dataset contains 91,479
human-annotated questions for 48,138 image sets, and the outdoor-scene dataset
has 49,617 questions for 12,746 image sets. We analyze the properties of the two
datasets, including question-and-answer distributions, types of questions,
biases in the dataset, and question-image dependencies. We also build new
baseline models to investigate new research challenges in ISVQA.</p>
<h2>Datasets</h2>
<p>Our datasets and more details can be found <a href="https://github.com/ankanbansal/ISVQA-Dataset">here</a>.</p>
<h2>Paper</h2>
<p>Our paper is available <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660052.pdf">here</a>.</p>
<p>If you use the datasets, please cite our paper using the bibtex:<br /></p>
<div class="codeblock">
<div class="blockcontent"><pre>
@inproceedings{bansal2018isvqa,
    author = {Bansal, Ankan and Zhang, Yuting and Chellappa, Rama},
    title = {Visual Question Answering on Image Sets},
    booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
    year = {2020},
}
</pre></div></div>
<div id="footer">
<div id="footer-text">
Page generated 2020-09-01 01:32:11 EDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
